{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import hmmlearn.hmm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "result = defaultdict(list)\n",
    "\n",
    "path_to_data = \"./Data_Filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path) # read .wav file\n",
    "    hop_length = math.floor(sr*0.010) # 10ms hop\n",
    "    win_length = math.floor(sr*0.025) # 25ms frame\n",
    "    # mfcc is 12 x T matrix\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y, sr, n_mfcc=12, n_fft=1024,\n",
    "        hop_length=hop_length, win_length=win_length)\n",
    "    # substract mean from mfcc --> normalize mfcc\n",
    "    mfcc = mfcc - np.mean(mfcc, axis=1).reshape((-1,1)) \n",
    "    # delta feature 1st order and 2nd order\n",
    "    delta1 = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # X is 36 x T\n",
    "    X = np.concatenate([mfcc, delta1, delta2], axis=0) # O^r\n",
    "    # return T x 36 (transpose of X)\n",
    "    return X.T # hmmlearn use T x N matrix\n",
    "\n",
    "def get_class_data(data_dir):\n",
    "    ls = os.listdir(data_dir)\n",
    "    files = [f for f in ls if f.endswith(\".wav\")]\n",
    "    random.shuffle(files)\n",
    "    mfcc = [get_mfcc(os.path.join(data_dir,f)) for f in files]\n",
    "    return mfcc\n",
    "\n",
    "def clustering(X, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=100, random_state=0, verbose=0)\n",
    "    kmeans.fit(X)\n",
    "    print(\"centers\", kmeans.cluster_centers_.shape)\n",
    "    return kmeans  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Nha dataset - 121\n",
      "Load ThanhPho dataset - 101\n",
      "Load Me dataset - 108\n",
      "Load YTe dataset - 123\n",
      "Load Hoc dataset - 108\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"Nha\", \"ThanhPho\",  \"Me\", \"YTe\", \"Hoc\",]# \"test_ThanhPho\", \"test_Me\", \"test_Nha\", \"test_YTe\", \"test_Hoc\",]\n",
    "\n",
    "datas = {}\n",
    "dataset = {}\n",
    "for cname in class_names:\n",
    "    print(f\"Load {cname} dataset\", end=' - ')\n",
    "    datas[cname] = get_class_data(os.path.join(path_to_data, cname))\n",
    "    print(len(datas[cname]))\n",
    "    datas[f\"test_{cname}\"] = datas[cname][-20:]\n",
    "    datas[cname] = datas[cname][:-20]\n",
    "    #datas[f\"test_{cname}\"] = get_class_data(os.path.join(path_to_data, f\"test_{cname}\"))\n",
    "    \n",
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_components = {\n",
    "    #   tʰa̤jŋ˨˩ fo˧˥ -> 5 âm vị -> 15 states\n",
    "    \"ThanhPho\": 15,\n",
    "    #  mɛ̰ʔ˨˩ -> 2 âm vị -> 6 states\n",
    "    \"Me\": 6,\n",
    "    #  i˧˧ te˧˥ -> 3 âm vị -> 9 states\n",
    "    \"YTe\": 9,\n",
    "    #  ha̰ʔwk˨ -> 3 âm vị -> 9 states\n",
    "    \"Hoc\": 9,\n",
    "    #  ɲa̤ː˨˩ -> 2 âm vị -> 6 states\n",
    "    \"Nha\": 6,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class Nha\n",
      "(15860, 36) [184, 140, 161, 220, 158, 181, 171, 143, 145, 140, 158, 156, 153, 149, 145, 171, 159, 173, 127, 122, 148, 140, 156, 212, 140, 225, 150, 168, 145, 179, 145, 152, 145, 155, 161, 186, 147, 99, 166, 145, 122, 186, 161, 163, 154, 145, 210, 158, 130, 143, 181, 189, 142, 137, 161, 135, 158, 168, 142, 148, 152, 154, 147, 186, 140, 152, 159, 166, 137, 145, 140, 152, 161, 132, 156, 137, 143, 173, 147, 140, 161, 143, 181, 168, 149, 145, 150, 132, 217, 130, 140, 207, 173, 168, 163, 179, 143, 155, 137, 204, 143] 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1    -1273450.8811             +nan\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for cname in class_names:\n",
    "    class_vectors = datas[cname]\n",
    "    # convert all vectors to the cluster index\n",
    "    # dataset['one'] = [O^1, ... O^R]\n",
    "    # O^r = (c1, c2, ... ct, ... cT)\n",
    "    # O^r size T x 1\n",
    "    km = kmeans\n",
    "    dataset[cname] = list([km.predict(v).reshape((-1,1)) for v in datas[cname]])\n",
    "    dataset[f\"test_{cname}\"] = list([km.predict(v).reshape((-1,1)) for v in datas[f\"test_{cname}\"]])\n",
    "\n",
    "    if cname[:4] != 'test':\n",
    "        n = dict_components[cname]\n",
    "        startprob = np.zeros(n)\n",
    "        startprob[0] = 1.0\n",
    "        transmat=np.diag(np.full(n,1))\n",
    "        #transmat = np.array(dict_transmat[cname])\n",
    "        \n",
    "        hmm = hmmlearn.hmm.GMMHMM(\n",
    "            n_components=n, \n",
    "            n_mix = 4, random_state=10, n_iter=500, verbose=True,\n",
    "            params='mctw', init_params='mct',\n",
    "            startprob_prior=startprob,\n",
    "            transmat_prior=transmat,\n",
    "        )\n",
    "    \n",
    "        X = np.concatenate(datas[cname])\n",
    "        lengths = list([len(x) for x in datas[cname]])\n",
    "        print(\"training class\", cname)\n",
    "        print(X.shape, lengths, len(lengths))\n",
    "        # FOR GMMHMM: NO NEED lengths parameter\n",
    "        hmm.fit(X)\n",
    "        models[cname] = hmm\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing\")\n",
    "result = {}\n",
    "for cname in class_names:\n",
    "    true_cname = f\"test_{cname}\"\n",
    "    true_predict = 0\n",
    "#     for O in dataset[true_cname]:\n",
    "    for O in datas[true_cname]:\n",
    "        score = {cname : model.score(O, [len(O)]) for cname, model in models.items()}\n",
    "        predict = max(score, key=score.get)\n",
    "        if predict == cname:\n",
    "            true_predict += 1\n",
    "#         print(true_cname, score, predict)\n",
    "    result[true_cname] = f\"QUANTITY: {true_predict}/{len(datas[true_cname])}\\nACCURACY: {100*true_predict/len(datas[true_cname])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in result.items():\n",
    "    print(k,'\\n',v,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "for k, v in models.items():\n",
    "    print(k,v.transmat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"m.pkl\", \"wb\") as file:\n",
    "    pickle.dump(models, file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
